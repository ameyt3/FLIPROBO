{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header tags from wikipedia main page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
    "soup=BeautifulSoup(page.content,'html.parser')\n",
    "tags=soup.find_all(['h1','h2','h3','h4','h5','h6'])\n",
    "tag=[]\n",
    "for i in tags:\n",
    "    tag.append(i)\n",
    "tags=pd.DataFrame({})\n",
    "tags['Tag']=tag\n",
    "tags.to_csv('wiki_tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB top moves page 1 and page 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    movie_name=[]\n",
    "    year=[]\n",
    "    rating=[]\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    titles=soup.find_all('h3',class_='lister-item-header')\n",
    "    for i in titles:\n",
    "        for j in i.find_all('a'):\n",
    "            movie_name.append(j.text)\n",
    "    \n",
    "    years=soup.find_all('span',class_='lister-item-year text-muted unbold')   \n",
    "    for i in years:\n",
    "        year.append(i.text)\n",
    "    \n",
    "    ratings=soup.find_all('div',class_='inline-block ratings-imdb-rating')\n",
    "    for i in ratings:\n",
    "        for j in i.find_all('strong'):\n",
    "            rating.append(j.text)\n",
    "    \n",
    "    movies=pd.DataFrame({})\n",
    "    movies['Name']=movie_name\n",
    "    movies['Year']=year\n",
    "    movies['rating']=rating\n",
    "    movies.head()\n",
    "    return movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_pg1=scrape('https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc')\n",
    "scrape_pg2=scrape('https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&start=51&ref_=adv_nxt')\n",
    "(scrape_pg1.append(scrape_pg2,ignore_index=True )).to_csv('movie_list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indian movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_indian(url):\n",
    "    movie_name=[]\n",
    "    year=[]\n",
    "    rating=[]\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    titles=soup.find_all('h3',class_='lister-item-header')\n",
    "    for i in titles:\n",
    "        for j in i.find_all('a'):\n",
    "            movie_name.append(j.text)\n",
    "    \n",
    "    years=soup.find_all('span',class_='lister-item-year text-muted unbold')   \n",
    "    for i in years:\n",
    "        year.append(i.text)\n",
    "    \n",
    "    ratings=soup.find_all('div',class_='ipl-rating-star small')\n",
    "    for i in ratings:\n",
    "        for j in i.find_all('span',class_='ipl-rating-star__rating'):\n",
    "            rating.append(j.text)\n",
    "    \n",
    "    movies_ind=pd.DataFrame({})\n",
    "    movies_ind['Name']=movie_name\n",
    "    movies_ind['Year']=year\n",
    "    movies_ind['rating']=rating\n",
    "    return movies_ind\n",
    "res=scrape_indian('https://www.imdb.com/list/ls056092300/')\n",
    "res.to_csv('top100indian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# book reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_info(url):\n",
    "    book_name=[]\n",
    "    author=[]\n",
    "    genre=[]\n",
    "    review=[]\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    names=soup.find_all('h4',class_='italic')\n",
    "    for i in names:\n",
    "        book_name.append(i.text.replace('\\n',''))\n",
    "    authors=soup.find_all('p',class_='sans bold')\n",
    "    for i in authors:\n",
    "        author.append(i.text.replace('\\n',''))\n",
    "    genres=soup.find_all('p',class_='genre-links hidden-phone')\n",
    "    for i in genres:\n",
    "        genre.append(i.text.replace('\\n',''))\n",
    "    reviews=soup.find_all('p',class_='excerpt')\n",
    "    for i in reviews:\n",
    "        review.append(i.text.replace('\\n',''))\n",
    "    books=pd.DataFrame({})\n",
    "    books['Name']=book_name\n",
    "    books['Author']=author\n",
    "    books['Genre']=genre\n",
    "    books['Review']=review\n",
    "    return books\n",
    "(book_info('https://bookpage.com/reviews')).to_csv('Book Reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# san francisco weather "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time=[]\n",
    "forecast=[]\n",
    "page=requests.get('https://forecast.weather.gov/MapClick.php?lat=37.777120000000025&lon=-122.41963999999996#.YAAeuugzaUk')\n",
    "soup=BeautifulSoup(page.content,'html.parser')\n",
    "titles=soup.find_all('div',class_='col-sm-2 forecast-label')\n",
    "for i in titles:\n",
    "    time.append(i.get_text())\n",
    "forecasts=soup.find_all('div',class_='col-sm-10 forecast-text')\n",
    "for i in forecasts:\n",
    "    forecast.append(i.get_text())\n",
    "weather=pd.DataFrame({})\n",
    "weather['Time']=time\n",
    "weather['Forecast']=forecast\n",
    "weather.to_csv('Weather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# phones under 20k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(r'chromedriver.exe')\n",
    "url='https://www.amazon.in/s?k=mobile+phones+under+20000&rh=p_36%3A-2000000&page=1&crid=K6WMQVL9S76S&qid=1610219994&rnid=1318502031&sprefix=mobile+phones+%2Caps%2C288'\n",
    "driver.get(url)\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "j=0\n",
    "urls=[]\n",
    "#scraping only 5 pages as there are almost 315 results and scraping one by one will take a lot of time\n",
    "while j<5:\n",
    "    time.sleep(5)\n",
    "    url_tags=driver.find_elements_by_xpath('//a[@class=\"a-link-normal a-text-normal\"]')\n",
    "    for url in url_tags:\n",
    "        i=url.get_attribute('href')\n",
    "        urls.append(i)\n",
    "    \n",
    "    driver.find_element_by_xpath('//li[@class=\"a-last\"]').click()\n",
    "    j+=1\n",
    "    print(j)\n",
    "name=[]\n",
    "price=[]\n",
    "img_link=[]\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        title=driver.find_element_by_xpath('//span[@class=\"a-size-large product-title-word-break\"]')\n",
    "        name.append(title.text)\n",
    "    except NoSuchElementException:\n",
    "        name.append('--')\n",
    "    try:\n",
    "        cost=driver.find_element_by_xpath('//span[@class=\"a-size-medium a-color-price priceBlockBuyingPriceString\"]')\n",
    "        price.append(cost.text)\n",
    "    except NoSuchElementException:\n",
    "        price.append('--')\n",
    "    try:\n",
    "        img=driver.find_element_by_xpath('//img[@class=\"imgTagWrapper\"]')\n",
    "        img_link.append(img.get_attribute('src'))\n",
    "    except:\n",
    "        img_link.append('NA')\n",
    "phones=pd.DataFrame({})\n",
    "phones['Name']=name\n",
    "phones['Price']=price\n",
    "phones['Image_Link']=img_link\n",
    "phones.to_csv('Phones_under_20k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# monster software developer jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'chromedriver.exe')\n",
    "url='https://www.monsterindia.com/srp/results?query=software%20developers&searchId=dd5f0cbd-b182-439c-ac5b-dece7607a9bd'\n",
    "driver.get(url)\n",
    "job_title=[]\n",
    "company=[]\n",
    "location=[]\n",
    "j=0\n",
    "while j<5:\n",
    "    time.sleep(5)\n",
    "    titles=driver.find_elements_by_xpath('//h3[@class=\"medium\"]')\n",
    "    for i in titles:\n",
    "        job_title.append(i.text)\n",
    "    \n",
    "    companies=driver.find_elements_by_xpath('//span[@class=\"company-name\"]')\n",
    "    for i in titles:\n",
    "        company.append(i.text)\n",
    "    \n",
    "    locations=driver.find_elements_by_xpath('//span[@class=\"loc\"]')\n",
    "    for i in titles:\n",
    "        location.append(i.text)\n",
    "    driver.find_element_by_xpath('//button[@class=\"btn-next-prev btn-next\"]').click()\n",
    "    j+=1\n",
    "jobs_monster=pd.DataFrame({})\n",
    "jobs_monster['JobTitle']=job_title\n",
    "jobs_monster['Company']=company\n",
    "\n",
    "jobs_monster.to_csv('monster_soft_developer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# monster delhi jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'chromedriver.exe')\n",
    "url='https://www.monsterindia.com/srp/results?query=Data%20Scientist&locations=Delhi&searchId=4b2da987-8b2b-480f-bbe5-cbd662f00c04'\n",
    "driver.get(url)\n",
    "job_title=[]\n",
    "company=[]\n",
    "location=[]\n",
    "j=0\n",
    "while j<5:\n",
    "    time.sleep(5)\n",
    "    titles=driver.find_elements_by_xpath('//h3[@class=\"medium\"]')\n",
    "    for i in titles:\n",
    "        job_title.append(i.text)\n",
    "    \n",
    "    companies=driver.find_elements_by_xpath('//span[@class=\"company-name\"]')\n",
    "    for i in companies:\n",
    "        company.append(i.text)\n",
    "    \n",
    "    locations=driver.find_elements_by_xpath('//span[@class=\"loc\"]')\n",
    "    for i in locations:\n",
    "        location.append(i.text)\n",
    "    driver.find_element_by_xpath('//button[@class=\"btn-next-prev btn-next\"]').click()\n",
    "    j+=1\n",
    "jobs_monster=pd.DataFrame({})\n",
    "jobs_monster['JobTitle']=job_title\n",
    "jobs_monster['Company']=company[0:125]\n",
    "\n",
    "jobs_monster.to_csv('monster_DS_delhi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
